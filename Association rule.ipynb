{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c999a30d-a746-425f-917f-95481bccdd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Downloading mlxtend-0.23.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.4.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (3.8.4)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->mlxtend) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
      "Downloading mlxtend-0.23.1-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.4 MB 653.6 kB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.1/1.4 MB 787.7 kB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.1/1.4 MB 901.1 kB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.2/1.4 MB 985.7 kB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.2/1.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.3/1.4 MB 983.0 kB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.3/1.4 MB 999.9 kB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.3/1.4 MB 855.7 kB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.3/1.4 MB 807.1 kB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.3/1.4 MB 807.1 kB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.3/1.4 MB 807.1 kB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.3/1.4 MB 807.1 kB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.3/1.4 MB 807.1 kB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.3/1.4 MB 807.1 kB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.3/1.4 MB 807.1 kB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 541.2 kB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 535.4 kB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 497.4 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 497.4 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 497.4 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.5/1.4 MB 462.0 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.5/1.4 MB 462.0 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.5/1.4 MB 433.9 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.5/1.4 MB 433.9 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 0.5/1.4 MB 407.5 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 0.5/1.4 MB 390.0 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.5/1.4 MB 391.7 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.5/1.4 MB 391.7 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.5/1.4 MB 388.6 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 0.6/1.4 MB 390.4 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 0.6/1.4 MB 376.4 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 0.6/1.4 MB 382.3 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 0.6/1.4 MB 381.2 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 0.6/1.4 MB 382.9 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 383.4 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 383.4 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 0.7/1.4 MB 380.3 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 0.7/1.4 MB 381.8 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.7/1.4 MB 392.1 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.7/1.4 MB 396.6 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 0.8/1.4 MB 396.5 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 405.6 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 411.6 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 0.9/1.4 MB 427.1 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.0/1.4 MB 448.1 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.4 MB 457.3 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 476.7 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.1/1.4 MB 489.3 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.4 MB 500.5 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.4 MB 500.5 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.4 MB 500.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.2/1.4 MB 495.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.2/1.4 MB 488.6 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.2/1.4 MB 486.5 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.4 MB 490.6 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.4 MB 485.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.3/1.4 MB 487.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.3/1.4 MB 485.8 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.4 MB 485.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 482.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 487.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 480.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.4 MB 480.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 473.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 473.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 473.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 473.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 473.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 473.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 473.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 473.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 424.6 kB/s eta 0:00:00\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.23.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mlxtend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da573281-18fb-4e7e-a2af-853bef42a486",
   "metadata": {},
   "source": [
    "Data Preprocessing:\n",
    "Pre-process the dataset to ensure it is suitable for Association rules, this may include handling missing values, removing duplicates, and converting the data to appropriate format.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "179bb0e0-55c0-4c2b-acc7-d2b74e956c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    asparagus  almonds  antioxydant juice  asparagus  avocado  babies food  \\\n",
      "0       False    False              False      False    False        False   \n",
      "1       False    False              False      False    False        False   \n",
      "2       False    False              False      False     True        False   \n",
      "3       False    False              False      False    False        False   \n",
      "4       False    False              False      False    False        False   \n",
      "\n",
      "   bacon  barbecue sauce  black tea  blueberries  ...  turkey  vegetables mix  \\\n",
      "0  False           False      False        False  ...   False           False   \n",
      "1  False           False      False        False  ...   False           False   \n",
      "2  False           False      False        False  ...    True           False   \n",
      "3  False           False      False        False  ...   False           False   \n",
      "4  False           False      False        False  ...   False           False   \n",
      "\n",
      "   water spray  white wine  whole weat flour  whole wheat pasta  \\\n",
      "0        False       False             False              False   \n",
      "1        False       False             False              False   \n",
      "2        False       False             False              False   \n",
      "3        False       False             False              False   \n",
      "4        False       False             False              False   \n",
      "\n",
      "   whole wheat rice   yams  yogurt cake  zucchini  \n",
      "0             False  False        False     False  \n",
      "1             False  False        False     False  \n",
      "2             False  False        False     False  \n",
      "3              True  False        False     False  \n",
      "4             False  False        False     False  \n",
      "\n",
      "[5 rows x 120 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = 'C:/Users/DELL/OneDrive/Documents/data science asignments/Association Rules/Association Rules/Online retail.csv'  # Update with your file path if different\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Step 2: Rename the column for easier handling and split items in each transaction\n",
    "data.columns = [\"Transaction\"]\n",
    "transactions = data[\"Transaction\"].str.split(',')\n",
    "\n",
    "# Step 3: Remove duplicates within each transaction and sort items\n",
    "transactions = transactions.apply(lambda x: sorted(set(x)))\n",
    "\n",
    "# Step 4: Remove empty transactions or those with NaN values\n",
    "transactions = transactions.dropna()\n",
    "\n",
    "# Step 5: Convert to transaction-item format\n",
    "# Using a dictionary to create a binary matrix\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Step 6: Use TransactionEncoder to encode data as a binary matrix\n",
    "te = TransactionEncoder()\n",
    "te_data = te.fit(transactions).transform(transactions)\n",
    "df_transactions = pd.DataFrame(te_data, columns=te.columns_)\n",
    "\n",
    "# Display the first few rows of the preprocessed dataset\n",
    "print(df_transactions.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab249027-51fe-4d8e-b912-67d37415914d",
   "metadata": {},
   "source": [
    "\n",
    "Association Rule Mining:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099eda6d-10cc-43c1-b662-258fc5a6a38b",
   "metadata": {},
   "source": [
    "â€¢\tImplement an Apriori algorithm using tool like python with libraries such as Pandas and Mlxtend etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6dfc63-bc79-42bf-8f00-94284a31f507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "       support                                 itemsets\n",
      "0    0.020267                                (almonds)\n",
      "1    0.033200                                (avocado)\n",
      "2    0.010800                         (barbecue sauce)\n",
      "3    0.014267                              (black tea)\n",
      "4    0.011467                             (body spray)\n",
      "..        ...                                      ...\n",
      "254  0.011067       (mineral water, milk, ground beef)\n",
      "255  0.017067  (mineral water, ground beef, spaghetti)\n",
      "256  0.015733         (mineral water, milk, spaghetti)\n",
      "257  0.010267    (mineral water, olive oil, spaghetti)\n",
      "258  0.011467     (spaghetti, mineral water, pancakes)\n",
      "\n",
      "[259 rows x 2 columns]\n",
      "\n",
      "Association Rules:\n",
      "            antecedents      consequents  antecedent support  \\\n",
      "0  (ground beef, eggs)  (mineral water)               0.020   \n",
      "1  (milk, ground beef)  (mineral water)               0.022   \n",
      "\n",
      "   consequent support   support  confidence      lift  leverage  conviction  \\\n",
      "0            0.238267  0.010133    0.506667  2.126469  0.005368    1.544054   \n",
      "1            0.238267  0.011067    0.503030  2.111207  0.005825    1.532756   \n",
      "\n",
      "   zhangs_metric  \n",
      "0       0.540548  \n",
      "1       0.538177  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "\n",
    "# Step 2: Split items in each transaction and preprocess data\n",
    "data.columns = [\"Transaction\"]\n",
    "transactions = data[\"Transaction\"].str.split(',').apply(lambda x: sorted(set(x)))\n",
    "transactions = transactions.dropna()\n",
    "\n",
    "# Step 3: Convert to transaction-item format using TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "te_data = te.fit(transactions).transform(transactions)\n",
    "df_transactions = pd.DataFrame(te_data, columns=te.columns_)\n",
    "\n",
    "# Step 4: Apply Apriori to find frequent itemsets\n",
    "# Set min_support based on your requirements (e.g., 0.01 for items occurring in 1% of transactions)\n",
    "min_support = 0.01\n",
    "frequent_itemsets = apriori(df_transactions, min_support=min_support, use_colnames=True)\n",
    "\n",
    "# Step 5: Generate association rules from the frequent itemsets\n",
    "# Set min_threshold for confidence (e.g., 0.5 for 50% confidence)\n",
    "min_confidence = 0.5\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "# Display results\n",
    "print(\"Frequent Itemsets:\\n\", frequent_itemsets)\n",
    "print(\"\\nAssociation Rules:\\n\", rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8785df8d-d57d-44fa-ab1b-f4d4b0536063",
   "metadata": {},
   "source": [
    "Apply association rule mining techniques to the pre-processed dataset to discover interesting relationships between products purchased together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfecfa31-25fd-4996-bd32-ad71a2a0cc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "       support                                 itemsets\n",
      "0    0.020267                                (almonds)\n",
      "1    0.033200                                (avocado)\n",
      "2    0.010800                         (barbecue sauce)\n",
      "3    0.014267                              (black tea)\n",
      "4    0.011467                             (body spray)\n",
      "..        ...                                      ...\n",
      "254  0.011067       (mineral water, milk, ground beef)\n",
      "255  0.017067  (mineral water, ground beef, spaghetti)\n",
      "256  0.015733         (mineral water, milk, spaghetti)\n",
      "257  0.010267    (mineral water, olive oil, spaghetti)\n",
      "258  0.011467     (spaghetti, mineral water, pancakes)\n",
      "\n",
      "[259 rows x 2 columns]\n",
      "\n",
      "Association Rules:\n",
      "            antecedents      consequents  antecedent support  \\\n",
      "0  (ground beef, eggs)  (mineral water)               0.020   \n",
      "1  (milk, ground beef)  (mineral water)               0.022   \n",
      "\n",
      "   consequent support   support  confidence      lift  leverage  conviction  \\\n",
      "0            0.238267  0.010133    0.506667  2.126469  0.005368    1.544054   \n",
      "1            0.238267  0.011067    0.503030  2.111207  0.005825    1.532756   \n",
      "\n",
      "   zhangs_metric  \n",
      "0       0.540548  \n",
      "1       0.538177  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "data.columns = [\"Transaction\"]\n",
    "transactions = data[\"Transaction\"].str.split(',').apply(lambda x: sorted(set(x)))\n",
    "transactions = transactions.dropna()\n",
    "\n",
    "# Step 2: Convert to Transaction-Item Matrix\n",
    "te = TransactionEncoder()\n",
    "te_data = te.fit(transactions).transform(transactions)\n",
    "df_transactions = pd.DataFrame(te_data, columns=te.columns_)\n",
    "\n",
    "# Step 3: Apply Apriori to Find Frequent Itemsets\n",
    "min_support = 0.01  # Set to your threshold\n",
    "frequent_itemsets = apriori(df_transactions, min_support=min_support, use_colnames=True)\n",
    "\n",
    "# Step 4: Generate Association Rules\n",
    "min_confidence = 0.5  # Set to your threshold\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "# Step 5: Filter Rules for High Lift and Confidence (optional)\n",
    "# E.g., filtering rules with lift > 1.2 to find meaningful patterns\n",
    "filtered_rules = rules[(rules['lift'] > 1.2) & (rules['confidence'] > min_confidence)]\n",
    "\n",
    "# Display the interesting rules discovered\n",
    "print(\"Frequent Itemsets:\\n\", frequent_itemsets)\n",
    "print(\"\\nAssociation Rules:\\n\", filtered_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5f01b6-97b5-4884-9ea2-76dbadf79168",
   "metadata": {},
   "source": [
    "â€¢\tSet appropriate threshold for support, confidence and lift to extract meaning full rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d87503-71e2-4652-bde3-a39ef1ce2a87",
   "metadata": {},
   "source": [
    "Threshold Recommendations\n",
    "Support: Represents the proportion of transactions containing a particular itemset. A good starting range is usually:\n",
    "\n",
    "min_support: Between 0.01 and 0.05 (1-5%), meaning the itemset appears in at least 1-5% of all transactions.\n",
    "If your dataset has many transactions, a higher support (e.g., 0.05) may filter out noise, but for smaller datasets, a lower support (e.g., 0.01) can capture more valuable rules.\n",
    "\n",
    "Confidence: Measures the likelihood that the presence of an itemset leads to the presence of another itemset.\n",
    "\n",
    "min_confidence: 0.6 - 0.8 (60-80%), depending on how strong you want the associations to be. Rules with confidence above 0.7 are generally reliable in retail datasets.\n",
    "Lift: Reflects the strength of a rule over random chance.\n",
    "\n",
    "lift > 1.2: Indicates the antecedent has a positive effect on the consequent (items are bought together more often than random chance).\n",
    "lift > 2: Highly associated itemsets. Set this threshold if looking for the strongest connections only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4705dda4-7e84-4d39-aeb1-67538be19526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "       support                           itemsets\n",
      "0    0.020267                          (almonds)\n",
      "1    0.033200                          (avocado)\n",
      "2    0.033733                         (brownies)\n",
      "3    0.087200                          (burgers)\n",
      "4    0.030133                           (butter)\n",
      "..        ...                                ...\n",
      "99   0.020133  (mineral water, whole wheat rice)\n",
      "100  0.022933             (olive oil, spaghetti)\n",
      "101  0.025200              (spaghetti, pancakes)\n",
      "102  0.021200                (shrimp, spaghetti)\n",
      "103  0.020933              (tomatoes, spaghetti)\n",
      "\n",
      "[104 rows x 2 columns]\n",
      "\n",
      "Association Rules:\n",
      " Empty DataFrame\n",
      "Columns: [antecedents, consequents, antecedent support, consequent support, support, confidence, lift, leverage, conviction, zhangs_metric]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "data.columns = [\"Transaction\"]\n",
    "transactions = data[\"Transaction\"].str.split(',').apply(lambda x: sorted(set(x)))\n",
    "transactions = transactions.dropna()\n",
    "\n",
    "# Convert to Transaction-Item Matrix\n",
    "te = TransactionEncoder()\n",
    "te_data = te.fit(transactions).transform(transactions)\n",
    "df_transactions = pd.DataFrame(te_data, columns=te.columns_)\n",
    "\n",
    "# Apply Apriori to Find Frequent Itemsets with min_support\n",
    "min_support = 0.02  # 2% support threshold\n",
    "frequent_itemsets = apriori(df_transactions, min_support=min_support, use_colnames=True)\n",
    "\n",
    "# Generate Association Rules with confidence and lift filtering\n",
    "min_confidence = 0.7  # 70% confidence threshold\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "# Filter for lift greater than 1.2 to find meaningful associations\n",
    "filtered_rules = rules[(rules['lift'] > 1.2) & (rules['confidence'] > min_confidence)]\n",
    "\n",
    "# Display the filtered, meaningful rules\n",
    "print(\"Frequent Itemsets:\\n\", frequent_itemsets)\n",
    "print(\"\\nAssociation Rules:\\n\", filtered_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36f7d5-c7e8-4e2b-8e18-51e1b0dfc88f",
   "metadata": {},
   "source": [
    "Analysis and Interpretation:\n",
    "â€¢\tAnalyse the generated rules to identify interesting patterns and relationships between the products.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83496ead-bb3f-437c-8bef-8a0713f4f489",
   "metadata": {},
   "source": [
    "To analyze and interpret the generated association rules, let's focus on key metrics â€” support, confidence, and lift â€” and what each reveals about the relationships between products. Here are some ways to approach the analysis and draw insights from the generated rules:\n",
    "\n",
    "1. Identify High-Support Rules:\n",
    "Support shows how frequently a ruleâ€™s itemset occurs in the dataset.\n",
    "Rules with high support indicate products frequently bought together, which could represent popular product bundles.\n",
    "Example Interpretation: If support for {Milk, Bread} -> {Butter} is 0.05 (5%), it means 5% of transactions contain this combination, suggesting that these items could be strategically positioned together in a store layout or promoted as a bundle.\n",
    "2. High-Confidence Rules:\n",
    "Confidence is the likelihood that the presence of an item (or items) in the antecedent leads to the presence of an item (or items) in the consequent.\n",
    "Rules with high confidence (e.g., above 70%) show strong predictive relationships. For example, {Diapers} -> {Baby Wipes} with high confidence would suggest that a significant proportion of customers buying diapers also buy baby wipes.\n",
    "Example Interpretation: A rule with 85% confidence for {Mineral Water} -> {Green Tea} indicates customers who buy mineral water often buy green tea as well, suggesting these items could be promoted together.\n",
    "3. High-Lift Rules:\n",
    "Lift measures the strength of association beyond random chance. Lift values above 1 indicate a positive association; the higher the lift, the stronger the association.\n",
    "Rules with high lift (e.g., >2) reveal items that significantly influence each otherâ€™s purchase likelihood, making them valuable for cross-promotions.\n",
    "Example Interpretation: If {Chicken} -> {Barbecue Sauce} has a lift of 2.5, it means customers buying chicken are 2.5 times more likely to buy barbecue sauce than by random chance. This insight could inform store layout or co-marketing strategies.\n",
    "4. Look for Complementary vs. Substitute Products:\n",
    "Some associations will reveal complementary products (items that go together) â€” for instance, {Coffee} -> {Sugar} or {Bread} -> {Butter}. These are ideal for cross-promotions or bundling.\n",
    "In other cases, rules may reveal substitute products â€” items purchased as alternatives (e.g., {Whole Milk} -> {Skim Milk}). Such insights can guide inventory management or pricing strategies to ensure both products meet customer demand without excessive overstock.\n",
    "5. Discover New Product Associations:\n",
    "Sometimes association rules reveal unexpected relationships. For example, {Avocado} -> {Green Tea} might indicate a health-focused shopping pattern, suggesting opportunities to create targeted marketing campaigns around wellness products.\n",
    "Unusual combinations that have high lift or confidence might reveal emerging customer preferences or seasonal trends (e.g., {Hot Chocolate} -> {Marshmallows} in winter).\n",
    "6. Seasonal or Context-Based Patterns:\n",
    "If possible, consider if certain rules may reflect seasonal patterns (e.g., {Ice Cream} -> {Cones} in summer) or specific event-based purchases (e.g., {Turkey} -> {Cranberry Sauce} near Thanksgiving).\n",
    "These insights are especially useful for planning seasonal promotions or special display setups.\n",
    "Example Summary of Insights:\n",
    "After analyzing the rules, you might summarize findings as follows:\n",
    "\n",
    "Product Bundling Opportunities: High-support and high-confidence rules suggest popular product combinations like {Bread, Butter}, which can be promoted together.\n",
    "Cross-Promotions: High-lift rules reveal strong relationships like {Pasta} -> {Tomato Sauce}, ideal for cross-promotional campaigns.\n",
    "Emerging Trends: Unexpected associations such as {Organic Juice} -> {Granola Bars} indicate growing preferences, valuable for targeted marketing.\n",
    "Incorporating these findings, a store could optimize product placement, develop targeted promotions, and even adapt inventory management to meet customer needs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2427f114-181b-4bec-bac6-cdbd8ac457ea",
   "metadata": {},
   "source": [
    "\tInterpret the results and provide insights into customer purchasing behaviour based on the discovered rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9053752-1425-42ad-a9a3-52a894d0f54c",
   "metadata": {},
   "source": [
    "Interpreting association rules provides valuable insights into customer purchasing behavior, helping us understand product relationships and tendencies in customer transactions. Based on common rule metrics (support, confidence, and lift), here are insights into typical customer behaviors you might discover and leverage from such an analysis:\n",
    "\n",
    "Insights into Customer Purchasing Behavior\n",
    "Frequent Product Pairings (High Support Rules):\n",
    "\n",
    "Observation: High-support rules show pairs or groups of products that appear in many transactions. These can represent essential or frequently purchased product combinations.\n",
    "Insight: If {Bread} -> {Butter} or {Milk, Eggs} -> {Cheese} are high-support rules, this suggests a core group of items customers often buy together. This insight could be used to create â€œstaple itemâ€ bundles to make shopping more convenient for customers.\n",
    "Actionable Strategy: Bundle these frequently bought items in special promotions or position them close together in the store to streamline customer experience.\n",
    "Predictive Buying Patterns (High Confidence Rules):\n",
    "\n",
    "Observation: High-confidence rules indicate that when customers buy one product, theyâ€™re highly likely to buy another specific product in the same transaction.\n",
    "Insight: Rules like {Mineral Water} -> {Green Tea} or {Diapers} -> {Baby Wipes} with high confidence imply that buying one item strongly suggests a need for the other. This behavior points to habitual or complementary buying patterns.\n",
    "Actionable Strategy: Use these insights to set up cross-promotions, like \"Buy Mineral Water, Get a Discount on Green Tea,\" or position these items close to each other to encourage combined purchases.\n",
    "Strongly Associated Products (High Lift Rules):\n",
    "\n",
    "Observation: Rules with high lift (>1) indicate that certain products are bought together significantly more often than random chance would predict.\n",
    "Insight: High-lift rules such as {Chicken} -> {Barbecue Sauce} with a lift of 2.5 reveal strong associations, possibly because they are used together or fit a specific need (e.g., preparing a barbecue meal). This signals that customers view these items as complements.\n",
    "Actionable Strategy: Promote these items together through special recipe-based marketing, seasonal displays, or meal-planning bundles to capitalize on these pairings.\n",
    "Complementary vs. Substitute Products:\n",
    "\n",
    "Observation: Complementary products show up in association rules where the purchase of one leads to the purchase of another (e.g., {Coffee} -> {Cream}), while substitutes may appear in alternative rules (e.g., {Whole Milk} -> {Skim Milk}).\n",
    "Insight: This pattern can help identify essentials vs. optional items, such as customers buying coffee alongside cream as a standard pair, whereas skim milk might substitute whole milk for different dietary needs.\n",
    "Actionable Strategy: Promote complementary items together for a one-stop purchase experience, while using inventory and pricing strategies for substitutes to offer choice without overstock.\n",
    "Discovery of New Trends or Emerging Preferences:\n",
    "\n",
    "Observation: Some association rules may reveal unexpected relationships, like {Avocado} -> {Granola Bars} or {Kale} -> {Smoothie Mix}, indicating specific dietary or lifestyle preferences.\n",
    "Insight: These unexpected product pairings may reflect shifts in customer preferences toward health-conscious or trend-based products.\n",
    "Actionable Strategy: Capitalize on these emerging trends with targeted ads, social media content, or by creating dedicated sections like â€œHealthy Choicesâ€ to cater to customer interests in a healthy lifestyle.\n",
    "Event or Season-Based Buying Patterns:\n",
    "\n",
    "Observation: Seasonal items may exhibit strong associations, such as {Hot Chocolate} -> {Marshmallows} during colder months, or {Grill Meats} -> {Charcoal} around barbecue season.\n",
    "Insight: Customers may buy specific items based on seasonal events or holidays, reflecting time-sensitive purchase patterns.\n",
    "Actionable Strategy: Anticipate and stock up on these seasonal items, offering timely promotions (e.g., â€œWinter Warmersâ€ or â€œSummer Barbecue Essentialsâ€) to increase sales during peak periods.\n",
    "Summary of Customer Purchasing Insights\n",
    "Bundling Opportunities: Frequently bought items and high-confidence rules can guide the creation of product bundles, enhancing convenience for customers and encouraging larger transactions.\n",
    "Cross-Promotion: Strong product associations (high lift) can inform cross-promotional campaigns, such as offering discounts on a complementary product.\n",
    "Inventory Management: Substitute product rules enable better inventory management and pricing strategies, helping to meet diverse customer preferences without excess stock.\n",
    "Trend and Season Insights: Unexpected associations and seasonal patterns help forecast demand and guide promotional campaigns, keeping the store stocked with trending items.\n",
    "By leveraging these insights, a business can improve the shopping experience through targeted promotions, better store layout, and optimized product pairings, ultimately increasing both customer satisfaction and sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294efecb-6536-4361-896c-4cd0337a0cf8",
   "metadata": {},
   "source": [
    "Interview Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e4943-bc2d-4d7b-aefb-c3a85fb827d8",
   "metadata": {},
   "source": [
    "\n",
    "1.\tWhat is lift and why is it important in Association rules?\n",
    "Lift is a measure in association rule mining that indicates the strength of a rule by comparing the observed frequency of itemsets occurring together with the frequency expected if the itemsets were independent. It is calculated as:\n",
    "\n",
    "Lift=ConfidenceÂ ofÂ theÂ Rule/SupportÂ ofÂ Consequent\n",
    "â€‹\n",
    " \n",
    "Or, equivalently:\n",
    "\n",
    "Lift(ð´â‡’ðµ)=Support(ð´âˆªðµ)/Support(ð´)Ã—Support(ðµ)\n",
    "\n",
    "â€‹Why itâ€™s important: Lift helps determine the strength of a rule beyond random chance:\n",
    "Lift > 1 indicates a positive association between items, meaning that the presence of one item increases the likelihood of the other.\n",
    "Lift = 1 implies no association between items; they appear together as often as expected by chance.\n",
    "Lift < 1 suggests a negative association, meaning items are less likely to appear together than random chance would predict.\n",
    "In summary: Lift is valuable in identifying rules that reveal meaningful relationships, guiding promotions and cross-selling strategies that highlight items with genuinely strong associations rather than merely popular items.\n",
    "\n",
    "\n",
    "2.\tWhat is support and Confidence. How do you calculate them?\n",
    "Definition: Support measures the frequency of an itemset in the dataset, representing the proportion of transactions where that itemset appears.\n",
    "Formula: \n",
    "Support(ð´)=NumberÂ ofÂ transactionsÂ containingÂ ð´/TotalÂ numberÂ ofÂ transactions\n",
    "â€‹\n",
    " \n",
    "Example: If an itemset {Milk, Bread} appears in 50 out of 1000 transactions, the support is \n",
    "50/1000 =0.05 or 5%.\n",
    "Importance: Support helps identify frequently occurring itemsets that have a significant presence in the dataset, allowing us to focus on more relevant item combinations.\n",
    "\n",
    "Confidence:\n",
    "\n",
    "Definition: Confidence is a measure of how often a ruleâ€™s consequent (right-hand item) appears in transactions that contain the ruleâ€™s antecedent (left-hand item).\n",
    "Formula: \n",
    "Confidence(ð´â‡’ðµ)=Support(ð´âˆªðµ)/Support(ð´)\n",
    "\n",
    "â€‹\n",
    " \n",
    "Example: If {Milk, Bread} appears in 50 transactions and {Milk} appears in 200 transactions, then \n",
    "Confidence\n",
    "(ð‘€ð‘–ð‘™ð‘˜â‡’ðµð‘Ÿð‘’ð‘Žð‘‘)=50/200=0.25 or 25%.\n",
    "Importance: Confidence indicates the likelihood of finding the consequent in transactions that already contain the antecedent, helping prioritize rules with stronger predictive power.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3.\tWhat are some limitations or challenges of Association rules mining?\n",
    "Association rule mining is a powerful tool for discovering relationships in data, but it comes with several challenges:\n",
    "\n",
    "High Computational Complexity:\n",
    "\n",
    "Challenge: As the number of items and transactions grows, the number of possible item combinations increases exponentially. This requires substantial computational resources, especially for large datasets with low support thresholds.\n",
    "Solution: Use optimized algorithms (like Apriori or FP-Growth) and appropriate thresholds to manage computation time and memory usage.\n",
    "Setting Optimal Thresholds:\n",
    "\n",
    "Challenge: Choosing appropriate support, confidence, and lift thresholds is not always straightforward. Setting thresholds too high can overlook valuable rules, while low thresholds can result in many irrelevant rules or â€œnoise.â€\n",
    "Solution: Experimenting with different thresholds and using domain knowledge can help fine-tune the values for meaningful results.\n",
    "Interpretability and Rule Redundancy:\n",
    "\n",
    "Challenge: Association rule mining often generates a large number of rules, many of which can be redundant or not meaningful. For example, rules with low lift may not offer new insights beyond item popularity.\n",
    "Solution: Filter rules based on metrics like lift or leverage to reduce redundancy and use visualization techniques to interpret complex patterns.\n",
    "Dealing with Rare but Important Items:\n",
    "\n",
    "Challenge: Association rule mining tends to favor frequent itemsets and may overlook less frequent but potentially valuable associations (e.g., rare or seasonal items).\n",
    "Solution: Adjust thresholds to allow rare item discovery, or use alternative metrics (like conviction) to focus on rare but impactful associations.\n",
    "Static Snapshot of Data:\n",
    "\n",
    "Challenge: Association rules often provide a static view of historical data and may not adapt well to trends or seasonal changes in customer behavior.\n",
    "Solution: Update the rules periodically and explore other techniques, like time-based analysis, to account for changing patterns over time.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
